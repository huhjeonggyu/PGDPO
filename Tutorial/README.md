# ğŸ PG-DPO Portfolio Optimization â€” Experiment Scripts

Welcome! ğŸ‰  
This repository is a **tutorial-style** collection of Python scripts that implement and test **Pontryagin-Guided Direct Policy Optimization (PG-DPO)** and its variants â€” step-by-step, from the most basic to more advanced forms.  
Itâ€™s designed not just for research, but also as a **hands-on learning resource** ğŸ“š so you can follow along, modify, and experiment.

---

## ğŸ§ª Test Environment

All experiments here are run on the **Kim and Omberg (1996)** continuous-time portfolio model ğŸ¦.  
This tutorial uses the simplest non-trivial setting â€” **one risky asset ğŸ’¹** and **one exogenous state variable ğŸ“ˆ** (the risk premium / Sharpe ratio X_t) â€” which makes it easier to understand while still capturing intertemporal hedging.

âœ¨ **Exogenous state dynamics** (risk premium OU process):  
`dX_t = -lambda_X * (X_t - X_bar) * dt + sigma_X * dZ_t^X`

ğŸ’° **Wealth dynamics**:  
`dW_t = r * W_t * dt + y_t * ( (mu_t - r) * dt + sigma_t * dZ_t )`

ğŸ¯ **Evaluation metrics**:  
- ğŸ“ RMSE between learned policy and the **closed-form Kimâ€“Omberg optimal policy**  
- ğŸ’¡ Expected utility difference from the analytical optimum

---

## ğŸ“‚ File Descriptions

### `pgdpo_base_single.py` â€” **Baseline PG-DPO**
ğŸš€ The starting point.  
Trains a Stage-1 PG-DPO policy **without** variance reduction or other enhancements. Reports:
- Stage-1 RMSE vs closed-form policy
- Expected utility difference vs closed-form

---

### `pgdpo_with_ppgdpo_single.py` â€” **P-PGDPO (Direct Costates)**
ğŸ¯ Takes the baseline policy and applies **PMP-based projection** using costates from BPTT.  
Shows how projection alone boosts accuracy.

---

### `pgdpo_antithetic_single.py` â€” **Antithetic PG-DPO**
ğŸ”„ Adds **antithetic sampling** to cut Monte Carlo variance in training and costate estimation.  
Improves both RMSE and utility.

**Snippet (minimal):**
```python
from torch.distributions import Normal

def simulate_with_sign(net_pi, T, W, dt, noise_sign=+1.0):
    logW = W.log(); sampler = Normal(0.0, 1.0)
    for k in range(m):
        t = k * dt
        state_t = torch.cat([logW.exp(), T - t], dim=1)
        pi_t = net_pi(state_t)
        mu_p = r + pi_t * (mu - r)
        sig2 = (pi_t * sigma)**2
        dZ = sampler.sample((len(W), 1)).to(dev) * noise_sign
        logW = logW + (mu_p - 0.5*sig2)*dt + torch.sqrt(sig2)*dZ*dt.sqrt()
        logW = logW.exp().clamp(min=lb_w).log()
    return logW.exp()

# paired evaluation (same CRNs, opposite signs)
W_T_pos = simulate_with_sign(net_pi, T, W, dt, +1.0)
W_T_neg = simulate_with_sign(net_pi, T, W, dt, -1.0)
U = 0.5 * (crra(W_T_pos) + crra(W_T_neg))  # lower-variance utility
```

- **Key:** Use the **same CRNs** with opposite signs and average. Apply to both Stageâ€‘1 and costate BPTT.

---

### `pgdpo_residual_single.py` â€” **Residual PG-DPO**
ğŸ›  Builds a residual network on top of the closed-form baseline, learning only **hedging demand corrections**.  
Training becomes more stable and RMSE drops dramatically.

**Snippet (minimal):**
```python
class ResidualPolicy(nn.Module):
    def __init__(self, base_policy):
        super().__init__()
        self.base = base_policy                  # closed-form or teacher
        self.res  = nn.Sequential(nn.Linear(2,128), nn.LeakyReLU(),
                                  nn.Linear(128,128), nn.LeakyReLU(),
                                  nn.Linear(128,1))
    def forward(self, state):                    # state = [W, X_t or Ï„]
        return self.base(state).detach() + self.res(state)  # Ï€ = Ï€_base + Î´Ï€_Î¸
```
- **Tip:** Detach the base (teacher/closedâ€‘form) so gradients update **only** the residual.

---

### `pgdpo_cv_single.py` â€” **Residual + Control Variate**
ğŸš Adds a **control variate** based on closed-form utility to residual PG-DPO.  
Further reduces variance and improves robustness under noisy conditions.

**Snippet (minimal):**
```python
@torch.no_grad()
def u_cf_same_crn(T, W, dt):
    return simulate_and_utility(closed_form_policy, T, W, dt)  # SAME CRNs

def cv_adjust(u_pol, u_cf):
    u_pol_c = u_pol - u_pol.mean()
    u_cf_c  = u_cf  - u_cf.mean()
    beta = (u_pol_c*u_cf_c).mean() / (u_cf_c.square().mean() + 1e-8)
    return (u_pol - beta*u_cf), beta

u_pol = simulate_and_utility(net_pi, T, W, dt)   # requires grad
u_cf  = u_cf_same_crn(T, W, dt)                  # no grad
u_adj, beta = cv_adjust(u_pol, u_cf)             # lower-variance target
loss = -u_adj.mean()
```
- **Key:** Use **identical CRNs** for policy and closed-form utility to maximize correlation.

---

### `pgdpo_richardson_single.py` â€” **Residual + CV + Richardson Extrapolation**
â© Adds **Richardson extrapolation** to the CV setup to reduce timestep bias in utility estimates.  
âš ï¸ In low-dimensional Kimâ€“Omberg, effect is minimal (and can be slightly negative) due to already tiny bias, but **in high dimensions it can shine** âœ¨.

**Snippet (minimal):**
```python
from torch.distributions import Normal

def util_with_steps(net_pi, T, W, dt, refine=1):
    sub_dt = dt / refine
    logW = W.log(); sampler = Normal(0.0, 1.0)
    for k in range(int(m*refine)):
        t = k * sub_dt
        state_t = torch.cat([logW.exp(), T - t], dim=1)
        pi_t = net_pi(state_t)
        mu_p = r + pi_t * (mu - r)
        sig2 = (pi_t * sigma)**2
        dZ = sampler.sample((len(W), 1)).to(dev)
        logW = logW + (mu_p - 0.5*sig2)*sub_dt + torch.sqrt(sig2)*dZ*sub_dt.sqrt()
        logW = logW.exp().clamp(min=lb_w).log()
    return crra(logW.exp())

U_coarse = util_with_steps(net_pi, T, W, dt, refine=1)
U_fine   = util_with_steps(net_pi, T, W, dt, refine=2)
U_star   = 2.0*U_fine - U_coarse                  # cancel O(dt) bias
loss = -U_star.mean()
```
- **Note:** Combine with antithetic/CV when timeâ€‘step bias is nonâ€‘negligible.

---

## ğŸ“Š Stage-by-Stage RMSE Results

| Variant                                   | Stage 1 RMSE | Stage 2 RMSE |
|-------------------------------------------|--------------|--------------|
| Baseline PG-DPO                           | 0.233300     | â€“            |
| Projected PG-DPO (P-PGDPO)                | 0.233300     | 0.005522     |
| P-PGDPO + Antithetic                      | 0.164274     | 0.004254     |
| P-PGDPO + Residual                        | 0.005663     | 0.003651     |
| P-PGDPO + Residual + Control Variate (CV) | 0.036626     | 0.003179     |
| P-PGDPO + Residual + CV + Richardson      | 0.037177     | 0.003411     |

---

### ğŸ“ Interpretation
- ğŸ“‰ **Stage-1 RMSE** drops sharply from BASE â†’ Residual.  
- ğŸ† **P-PGDPO projection** consistently gives RMSE < 0.006 across all variants.  
- ğŸ’¡ Antithetic and Residual improve Stage-1 accuracy well beyond BASE.  
- â³ CV and Richardson are more valuable in **high-dimensional problems**.

---

ğŸ’¡ **Note:** While this repo is research-grade, itâ€™s deliberately structured as a **learning-friendly tutorial** â€” you can run each script independently, compare results, and see exactly how each enhancement changes the outcome.
